{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdf1b2-9276-41ac-b06c-a3822d75b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd, numpy as np\n",
    "import os, time, pickle\n",
    "\n",
    "os.chdir('/home/jovyan/work/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656644b-76b9-4f5c-bd0a-3eacc5808b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c485406-536c-4593-b9f0-a4020f7381df",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdatadir = os.path.join('personas','mistraldata_llm_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8aa168-dc94-43f3-b17f-5d33a3d44771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import outlines\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226400fc-3da0-4aef-ab1f-9bf4ceab1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt, generator, verbose = False):\n",
    "    answer = generator(prompt)\n",
    "    if verbose == True:\n",
    "        print(prompt)\n",
    "        print(answer)\n",
    "    return answer   \n",
    "\n",
    "def load_llm(model_, choices):\n",
    "    llm = Llama(\n",
    "        model_path = 'models/'+model_[1],\n",
    "        n_gpu_layers = -1, \n",
    "        logits_all = False,\n",
    "        verbose = False,\n",
    "        # temp = 0\n",
    "    )\n",
    "    model = outlines.models.LlamaCpp(llm)\n",
    "    generator = outlines.generate.choice(model, choices)\n",
    "    return generator\n",
    "\n",
    "def load_model(model_):\n",
    "    llm = Llama(\n",
    "        model_path = 'models/'+model_[1],\n",
    "        n_gpu_layers = -1,\n",
    "        logits_all = False,\n",
    "        verbose = False,\n",
    "        # temp = 0\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def annotate_data(prompts, generator):\n",
    "    llm_labels = []\n",
    "    for prompt in prompts:\n",
    "        llm_labels.append(get_prediction(prompt, generator))\n",
    "    return llm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b97aa8-be1c-4f6a-8e86-61f32fba27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_dir = 'ggml-mistral-model-7B-f16.gguf'\n",
    "mistral_name = 'Mistral_7B'\n",
    "mistral_ = (mistral_name, mistral_dir)\n",
    "\n",
    "choices_binary = ['TRUE', 'FALSE']\n",
    "choices_scale = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fcaab-3460-40e7-955e-d0c1fbf78c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_scale = load_llm(mistral_, choices_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38824c-2660-41d5-8cc5-ec00c3748bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_mistral_persona_binary(persona, post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below for toxicity, taking on the perspective of the following persona: {persona}. Respond with 'TRUE' if this post seems toxic/hateful/disrespectful to you and 'FALSE' if this post does not seem toxic/hateful/disrespectful to you. Respond with 'TRUE' or 'FALSE' only.\\n<POST> {post}\\n<RESPONSE> \"\n",
    "\n",
    "def prompt_mistral_nopersona_binary(post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below for toxicity. Respond with 'TRUE' if this post seems toxic/hateful/disrespectful to you and 'FALSE' if this post does not seem toxic/hateful/disrespectful to you.\\n<POST> {post}\\n<RESPONSE> \"\n",
    "\n",
    "\n",
    "def prompt_mistral_persona_scale(persona, post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below, taking on the perspective of the following persona: {persona}. Answer the following question on a scale from 1 to 5, where 1 stands for 'Not at all' and 5 stands for 'Very much so'. How toxic/hateful/disrespectful or offensive does this post seem to you?\\n<POST> {post}\\n<RESPONSE> \"\n",
    "\n",
    "def prompt_mistral_nopersona_scale(post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below. Answer the following question on a scale from 1 to 5, where 1 stands for 'Not at all' and 5 stands for 'Very much so'. How toxic/hateful/disrespectful or offensive does this post seem to you?\\n<POST> {post}\\n<RESPONSE> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8e26f-0159-4a4b-847e-e1980c1eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.read_pickle('personas/data_ext/data_study_4.pkl')\n",
    "\n",
    "with open('personas/data_ext/dict_annotators.pkl', 'rb') as f:\n",
    "    dict_annotators = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00055ab4-83b6-4921-8461-c2a96b50b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in dict_annotators.items():\n",
    "    print(k,len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e8d94-c4f9-48d6-90fe-eaf98ed5efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_annotators['neutral_black']['persona'] = [t.replace('an african-american','a black').replace('african-american', 'black') for t in dict_annotators['neutral_black']['persona']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3953b0-742f-45fa-9145-d95c02c596fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 5\n",
    "batch_size = 50\n",
    "start_time = time.time()\n",
    "\n",
    "prompt_func = prompt_mistral_persona_scale\n",
    "\n",
    "for i in range(n_batches): # 5 batches a 50 personas\n",
    "    k_count = 0\n",
    "    for k,v in dict_annotators.items(): # 3 different dicts\n",
    "        personas = zip(list(v['personaId'])[i*batch_size:(i+1)*batch_size], list(v['persona'])[i*batch_size:(i+1)*batch_size])\n",
    "        if i == 0:\n",
    "            pd_ = pd_data.copy()\n",
    "        else:\n",
    "            pd_ = pd.read_pickle(os.path.join(outdatadir,f'{k}_annotations.pkl'))\n",
    "        texts = list(pd_['text'])\n",
    "        for persona in personas:\n",
    "            list_prompts = [prompt_func(persona[1], text) for text in texts]\n",
    "            pd_[f'persona_{persona[0]}'] = annotate_data(list_prompts, generator_scale)\n",
    "        pd_.to_pickle(os.path.join(outdatadir,f'{k}_annotations.pkl'))\n",
    "        k_count += 1\n",
    "        n_done = i * batch_size * len(dict_annotators.keys()) + k_count * batch_size\n",
    "        n_todo = n_batches * batch_size * len(dict_annotators.keys()) - n_done\n",
    "        time_done = time.time() - start_time\n",
    "        time_todo = (time_done / n_done) * n_todo\n",
    "        with open('personas/mistraldata_llm_4/monitor.txt', 'a') as f: # after each batch of 50 personas\n",
    "            f.write(f'done: {n_done}, to-do: {n_todo}, time elapsed: {np.round(time_done/(60*60),4)}h, eta: {np.round(time_todo/(60*60),4)}h\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
