{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdf1b2-9276-41ac-b06c-a3822d75b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd, numpy as np\n",
    "import os, time, pickle\n",
    "\n",
    "os.chdir('/home/jovyan/work/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656644b-76b9-4f5c-bd0a-3eacc5808b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8aa168-dc94-43f3-b17f-5d33a3d44771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import outlines\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226400fc-3da0-4aef-ab1f-9bf4ceab1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt, generator, verbose = False):\n",
    "    answer = generator(prompt)\n",
    "    if verbose == True:\n",
    "        print(prompt)\n",
    "        print(answer)\n",
    "    return answer   \n",
    "\n",
    "def load_llm(model_, choices):\n",
    "    llm = Llama(\n",
    "        model_path = 'models/'+model_[1],\n",
    "        n_gpu_layers = -1, \n",
    "        logits_all = False,\n",
    "        verbose = False,\n",
    "        # temp = 0\n",
    "    )\n",
    "    model = outlines.models.LlamaCpp(llm)\n",
    "    generator = outlines.generate.choice(model, choices)\n",
    "    return generator\n",
    "\n",
    "def load_model(model_):\n",
    "    llm = Llama(\n",
    "        model_path = 'models/'+model_[1],\n",
    "        n_gpu_layers = -1,\n",
    "        logits_all = False,\n",
    "        verbose = False,\n",
    "        # temp = 0\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def annotate_data(prompts, generator):\n",
    "    llm_labels = []\n",
    "    for prompt in prompts:\n",
    "        llm_labels.append(get_prediction(prompt, generator))\n",
    "    return llm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cc517-0881-4288-9c5d-ce09d029d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen2_dir = 'Qwen2-7B-Instruct-F16.gguf'\n",
    "qwen2_name = 'qwen2'\n",
    "qwen2_ = (qwen2_name, qwen2_dir)\n",
    "\n",
    "choices_binary = ['TRUE', 'FALSE']\n",
    "choices_scale = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c652d6f-c21d-4da1-98c5-bf50c797d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_binary = load_llm(qwen2_, choices_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b97aa8-be1c-4f6a-8e86-61f32fba27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_dir = 'ggml-qwen1.6-32-model-Q4_K_M.gguf'\n",
    "# qwen_name = 'qwen1.6'\n",
    "# qwen_ = (qwen_name, qwen_dir)\n",
    "\n",
    "# choices_binary = ['TRUE', 'FALSE']\n",
    "# choices_scale = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fcaab-3460-40e7-955e-d0c1fbf78c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_binary = load_llm(qwen_, choices_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38824c-2660-41d5-8cc5-ec00c3748bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_qwen_persona_binary(persona, post):\n",
    "    return f\"Your task is to annotate the post shown below for toxicity, taking on the perspective of the following persona: {persona}. Respond with 'TRUE' if this post seems toxic/hateful/disrespectful to you and 'FALSE' if this post does not seem toxic/hateful/disrespectful to you. Respond with 'TRUE' or 'FALSE' only, do not add any other explanations or descriptions.\\nPost: {post}\\nResponse:\"\n",
    "\n",
    "def prompt_qwen_nopersona_binary(post):\n",
    "    return f\"Your task is to annotate the post shown below for toxicity. Respond with 'TRUE' if this post seems toxic/hateful/disrespectful to you and 'FALSE' if this post does not seem toxic/hateful/disrespectful to you. Respond with 'TRUE' or 'FALSE' only, do not add any other explanations or descriptions.\\nPost: {post}\\nResponse:\"\n",
    "\n",
    "\n",
    "def prompt_qwen_persona_scale(persona, post):\n",
    "    return f\"Your task is to annotate the post shown below for toxicity, taking on the perspective of the following persona: {persona}. Answer the following question on a scale from 1 to 5, where 1 stands for 'Not at all' and 5 stands for 'Very much so'. How toxic/hateful/disrespectful or offensive does this post seem to you? Respond with an integer from 1 to 5 only, do not add any other explanations or descriptions.\\nPost: {post}\\nResponse:\"\n",
    "\n",
    "def prompt_qwen_nopersona_scale(post):\n",
    "    return f\"Your task is to annotate the post shown below for toxicity. Answer the following question on a scale from 1 to 5, where 1 stands for 'Not at all' and 5 stands for 'Very much so'. How toxic/hateful/disrespectful or offensive does this post seem to you? Respond with an integer from 1 to 5 only, do not add any other explanations or descriptions.\\nPost: {post}\\nResponse:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c072e-333f-4319-8744-1ef6c425b4d7",
   "metadata": {},
   "source": [
    "### 1000 persona sample annotations on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f9dd6-9611-42b0-82a0-4a6f700b631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {\n",
    "    'personal_you': {\n",
    "        'sample_personas': None,\n",
    "        'n': None,\n",
    "        'prompt_func': prompt_qwen_persona_binary,\n",
    "        'generator': generator_binary,\n",
    "        'out_dir': 'qwendata_llm_1_2',\n",
    "        'out_file': '1000_persona_you'\n",
    "    },\n",
    "    'unpersonal_you': {\n",
    "        'sample_personas': None,\n",
    "        'n': 50,\n",
    "        'prompt_func': prompt_qwen_nopersona_binary,\n",
    "        'generator': generator_binary,\n",
    "        'out_dir': 'qwendata_llm_1_2',\n",
    "        'out_file': '1000_nopersona_you'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7c20a-3992-4f3e-9e45-ec8dc459b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(pd_, pd_restart, sample_personas, n, prompt_func, generator, out_dir, out_file):\n",
    "    \n",
    "    if isinstance(pd_restart, pd.DataFrame):\n",
    "        pd_results = pd_restart.copy()\n",
    "    else:\n",
    "        pd_results = pd_.copy()\n",
    "\n",
    "    if not n == None: # if random run\n",
    "        all_prompts = [prompt_func(text) for text in list(pd_results['text'])]\n",
    "        adder = int(pd_results.columns[-1].split('_')[1]) if len(pd_results.columns)>3 else 0\n",
    "    else: # if persona run\n",
    "        all_prompts = {}\n",
    "\n",
    "    for i in range(len(sample_personas)):\n",
    "        if n == None: # if persona run\n",
    "            list_prompts = [prompt_func(sample_personas['persona'].values[i], text) for text in list(pd_results['text'])]     \n",
    "            persona_ix = sample_personas['personaId'].values[i]\n",
    "            pd_results[f'persona_{persona_ix}'] = annotate_data(list_prompts, generator)\n",
    "            all_prompts[persona_ix] = list_prompts\n",
    "        else: # if random run\n",
    "            pd_results[f'run_{i+1+adder}'] = annotate_data(all_prompts, generator)\n",
    "\n",
    "    if not os.path.exists(os.path.join('personas',out_dir)):\n",
    "        os.mkdir(os.path.join('personas',out_dir))\n",
    "    pd_results.to_pickle(os.path.join('personas',out_dir,out_file+'.pkl'))\n",
    "    \n",
    "    return pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8e26f-0159-4a4b-847e-e1980c1eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.read_pickle('personas/data_ext/lscale_majVote.pkl')\n",
    "pd_personas = pd.read_pickle('personas/data_ext/sample_1000_personas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eda019-d5c9-47b8-b5b3-10780425e863",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "batch_size = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_batches):\n",
    "    for k,v in run_params.items():\n",
    "        \n",
    "        if i == 0:\n",
    "            pd_restart = None\n",
    "        else:\n",
    "            pd_restart = pd.read_pickle(os.path.join('personas',v['out_dir'],v['out_file']+'.pkl'))\n",
    "            \n",
    "        v['sample_personas'] = pd_personas.iloc[(i*batch_size):(i+1)*batch_size,:]\n",
    "        \n",
    "        %time pd_results = run_llm(pd_data, pd_restart, v['sample_personas'], v['n'], v['prompt_func'], v['generator'], v['out_dir'], v['out_file'])\n",
    "        with open('personas/'+v['out_dir']+'/monitor.txt', 'a') as f:\n",
    "            f.write(f'done: {batch_size*(i+1)}, to-do: {1000-batch_size*(i+1)}, time elapsed: {np.round((time.time()-start_time)/(60*60),4)}h, eta: {(np.round((time.time()-start_time)/(60*60),4)/(batch_size*(i+1)))*(1000-batch_size*(i+1))}h\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55728e-a969-4453-afb9-a6ca003e09aa",
   "metadata": {},
   "source": [
    "### annotations for performance bracket personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515cd4f-c988-432f-a0af-13b246c7de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.read_pickle('personas/data_ext/lscale_majVote.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5554dd-53cd-4d53-9541-085e03845e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_personas = pd.read_pickle('personas/data_ext/pd_personas_cleaned.pkl')\n",
    "\n",
    "pd_brackets = pd.read_pickle('personas/qwen_study_1_2/performance_brackets.pkl')\n",
    "pd_brackets = pd_brackets.astype({'personaId': int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dae95-a0fc-43aa-b81a-4f1d67205de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_brackets = pd_brackets.merge(pd_personas, on='personaId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f9e75-a843-4d71-9875-3fddf0611473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_brackets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa48f63-f05b-443c-8c25-54dcc912b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bracket_results = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(30):\n",
    "    for m in range(3):\n",
    "        pid, persona = pd_brackets['personaId'][i+30*m], pd_brackets['persona'][i+30*m]\n",
    "        prompts = [prompt_qwen_persona_binary(persona, post) for post in pd_data['text']]\n",
    "        annotations = pd_data.copy()\n",
    "        for r in range(30):\n",
    "            annotations[f'run_{r}'] = annotate_data(prompts, generator_binary)\n",
    "        bracket_results[pid] = annotations\n",
    "        with open('personas/qwen_study_1_2/performance_brackets_results.pkl', 'wb') as f:\n",
    "            pickle.dump(bracket_results, f)\n",
    "        with open('personas/qwen_study_1_2/bracket_monitor.txt', 'a') as f:\n",
    "            f.write(f'done: {30*(i+1)}, to-do: {900-30*(i+1)}, time elapsed: {np.round((time.time()-start_time)/(60*60),4)}h, eta: {(np.round((time.time()-start_time)/(60*60),4)/(300*(i+1)))*(900-30*(i+1))}h\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
