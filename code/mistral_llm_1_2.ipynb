{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdf1b2-9276-41ac-b06c-a3822d75b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd, numpy as np\n",
    "import os, time, pickle\n",
    "\n",
    "os.chdir('/home/jovyan/work/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656644b-76b9-4f5c-bd0a-3eacc5808b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8aa168-dc94-43f3-b17f-5d33a3d44771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import outlines\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226400fc-3da0-4aef-ab1f-9bf4ceab1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt, generator, verbose = False):\n",
    "    answer = generator(prompt)\n",
    "    if verbose == True:\n",
    "        print(prompt)\n",
    "        print(answer)\n",
    "    return answer   \n",
    "\n",
    "def load_llm(model_, choices):\n",
    "    llm = Llama(\n",
    "        model_path = 'models/'+model_[1],\n",
    "        n_gpu_layers = -1, \n",
    "        logits_all = False,\n",
    "        verbose = False,\n",
    "        # temp = 0\n",
    "    )\n",
    "    model = outlines.models.LlamaCpp(llm)\n",
    "    generator = outlines.generate.choice(model, choices)\n",
    "    return generator\n",
    "\n",
    "def load_model(model_):\n",
    "    llm = Llama(\n",
    "        model_path = 'models/'+model_[1],\n",
    "        n_gpu_layers = -1,\n",
    "        logits_all = False,\n",
    "        verbose = False,\n",
    "        # temp = 0\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def annotate_data(prompts, generator):\n",
    "    llm_labels = []\n",
    "    for prompt in prompts:\n",
    "        llm_labels.append(get_prediction(prompt, generator))\n",
    "    return llm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b97aa8-be1c-4f6a-8e86-61f32fba27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_dir = 'ggml-mistral-model-7B-f16.gguf'\n",
    "mistral_name = 'Mistral_7B'\n",
    "mistral_ = (mistral_name, mistral_dir)\n",
    "\n",
    "choices_binary = ['TRUE', 'FALSE']\n",
    "choices_scale = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fcaab-3460-40e7-955e-d0c1fbf78c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_binary = load_llm(mistral_, choices_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38824c-2660-41d5-8cc5-ec00c3748bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_mistral_persona_binary(persona, post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below for toxicity, taking on the perspective of the following persona: {persona}. Respond with 'TRUE' if this post seems toxic/hateful/disrespectful to you and 'FALSE' if this post does not seem toxic/hateful/disrespectful to you. Respond with 'TRUE' or 'FALSE' only.\\n<POST> {post}\\n<RESPONSE> \"\n",
    "\n",
    "def prompt_mistral_nopersona_binary(post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below for toxicity. Respond with 'TRUE' if this post seems toxic/hateful/disrespectful to you and 'FALSE' if this post does not seem toxic/hateful/disrespectful to you.\\n<POST> {post}\\n<RESPONSE> \"\n",
    "\n",
    "\n",
    "def prompt_mistral_persona_scale(persona, post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below, taking on the perspective of the following persona: {persona}. Answer the following question on a scale from 1 to 5, where 1 stands for 'Not at all' and 5 stands for 'Very much so'. How toxic/hateful/disrespectful or offensive does this post seem to you?\\n<POST> {post}\\n<RESPONSE> \"\n",
    "\n",
    "def prompt_mistral_nopersona_scale(post):\n",
    "    return f\"<TASK> Your task is to annotate the post shown below. Answer the following question on a scale from 1 to 5, where 1 stands for 'Not at all' and 5 stands for 'Very much so'. How toxic/hateful/disrespectful or offensive does this post seem to you?\\n<POST> {post}\\n<RESPONSE> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f9dd6-9611-42b0-82a0-4a6f700b631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {\n",
    "    'persona': {\n",
    "        'sample_personas': None,\n",
    "        'n': None,\n",
    "        'prompt_func': prompt_mistral_persona_binary,\n",
    "        'generator': generator_binary,\n",
    "        'out_dir': 'mistraldata_llm_1_2',\n",
    "        'out_file': '1000_persona_you'\n",
    "    },\n",
    "    'nopersona': {\n",
    "        'sample_personas': None,\n",
    "        'n': 50,\n",
    "        'prompt_func': prompt_mistral_nopersona_binary,\n",
    "        'generator': generator_binary,\n",
    "        'out_dir': 'mistraldata_llm_1_2',\n",
    "        'out_file': '1000_nopersona_you'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7c20a-3992-4f3e-9e45-ec8dc459b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(pd_, pd_restart, sample_personas, n, prompt_func, generator, out_dir, out_file):\n",
    "    \n",
    "    if isinstance(pd_restart, pd.DataFrame):\n",
    "        pd_results = pd_restart.copy()\n",
    "    else:\n",
    "        pd_results = pd_.copy()\n",
    "\n",
    "    if not n == None: # if random run\n",
    "        all_prompts = [prompt_func(text) for text in list(pd_results['text'])]\n",
    "        adder = int(pd_results.columns[-1].split('_')[1]) if len(pd_results.columns)>3 else 0\n",
    "    else: # if persona run\n",
    "        all_prompts = {}\n",
    "\n",
    "    for i in range(len(sample_personas)):\n",
    "        if n == None: # if persona run\n",
    "            list_prompts = [prompt_func(sample_personas['persona'].values[i], text) for text in list(pd_results['text'])]     \n",
    "            persona_ix = sample_personas['personaId'].values[i]\n",
    "            pd_results[f'persona_{persona_ix}'] = annotate_data(list_prompts, generator)\n",
    "            all_prompts[persona_ix] = list_prompts\n",
    "        else: # if random run\n",
    "            pd_results[f'run_{i+1+adder}'] = annotate_data(all_prompts, generator)\n",
    "\n",
    "    if not os.path.exists(os.path.join('personas',out_dir)):\n",
    "        os.mkdir(os.path.join('personas',out_dir))\n",
    "    pd_results.to_pickle(os.path.join('personas',out_dir,out_file+'.pkl'))\n",
    "    \n",
    "    return pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8e26f-0159-4a4b-847e-e1980c1eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.read_pickle('personas/data_ext/lscale_majVote.pkl')\n",
    "pd_personas = pd.read_pickle('personas/data_ext/sample_1000_personas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68500b63-aeec-474f-a06e-0db792bf3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(20):\n",
    "    for k,v in run_params.items():\n",
    "        \n",
    "        if i == 0:\n",
    "            pd_restart = None\n",
    "        else:\n",
    "            pd_restart = pd.read_pickle(os.path.join('personas',v['out_dir'],v['out_file']+'.pkl'))\n",
    "            \n",
    "        v['sample_personas'] = pd_personas.iloc[(i*50):(i+1)*50,:]\n",
    "        \n",
    "        %time pd_results = run_llm(pd_data, pd_restart, v['sample_personas'], v['n'], v['prompt_func'], v['generator'], v['out_dir'], v['out_file'])\n",
    "        with open('personas/'+v['out_dir']+'/new_monitor.txt', 'a') as f:\n",
    "            f.write(f'done: {50*(i+1)}, to-do: {1000-50*(i+1)}, time elapsed: {np.round((time.time()-start_time)/(60*60),4)}h, eta: {(np.round((time.time()-start_time)/(60*60),4)/(50*(i+1)))*(1000-50*(i+1))}h\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
